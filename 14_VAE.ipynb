{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "14_VAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXR+MHvO/sTc2OHZAp4KbW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2021-stats2/blob/main/14_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta5z3DXdBxVE"
      },
      "source": [
        "# 変分オートエンコーダの実践\n",
        "* MNISTデータセットに対して変分オートエンコーダを適用してみる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6NYxEMd5pw4"
      },
      "source": [
        "## 準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiDT3AucqNZV"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import plotly.express as px\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMzNXboyCW0Z"
      },
      "source": [
        "## PyTorchの準備\n",
        "* PyTorchについての細かな説明は割愛します・・・。\n",
        " * https://github.com/pytorch/examples/blob/master/vae/main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuvJmN0-CQJl"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yhd-tM8tDzeC"
      },
      "source": [
        "torch.manual_seed(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-ADrejpGm1y"
      },
      "source": [
        "* GPUが使えるときは使う。\n",
        " * ランタイムのタイプをGPUにしておく。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGW9P7CFD2Yt"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0Y4XW9PD8so"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFJfRZH5ELGO"
      },
      "source": [
        "## MNISTデータを取得\n",
        "* PyTorchに用意されている仕組みを使ってデータを取得し、学習に使える状態にする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ontWmLh2EKy6"
      },
      "source": [
        "batch_size = 200\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EbD8TdRDHAO"
      },
      "source": [
        "## エンコーダとデコーダの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIvCjTp5CbcE"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "  def __init__(self, z_dim=20):\n",
        "    super(VAE, self).__init__()\n",
        "    # ここからエンコーダ\n",
        "    self.fc1 = nn.Linear(784, 400)\n",
        "    self.fc21 = nn.Linear(400, z_dim) # mean\n",
        "    self.fc22 = nn.Linear(400, z_dim) # log var\n",
        "    ## ここからデコーダ\n",
        "    self.fc3 = nn.Linear(z_dim, 400)\n",
        "    self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "  def encode(self, x):\n",
        "    h1 = F.relu(self.fc1(x))\n",
        "    return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "  def reparameterize(self, mu, logvar):\n",
        "    std = torch.exp(0.5*logvar)\n",
        "    eps = torch.randn_like(std)\n",
        "    return mu + eps*std\n",
        "\n",
        "  def decode(self, z):\n",
        "    h3 = F.relu(self.fc3(z))\n",
        "    return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mu, logvar = self.encode(x.view(-1, 784))\n",
        "    z = self.reparameterize(mu, logvar)\n",
        "    return self.decode(z), mu, logvar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7v6Y_yeEq9y"
      },
      "source": [
        "## 学習の準備\n",
        "* モデルのインスタンスを作成\n",
        "* オプティマイザを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lcuq2wvsDKr4"
      },
      "source": [
        "model = VAE(10).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLhXD1aWE4rm"
      },
      "source": [
        "## 損失関数を定義\n",
        "* ELBOにマイナスをつけたものの前半（データ尤度の項）と後半（KL情報量の項）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMt_vP2nEzOR"
      },
      "source": [
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "  BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "\n",
        "  # see Appendix B from VAE paper:\n",
        "  # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
        "  # https://arxiv.org/abs/1312.6114\n",
        "  # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "  KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "  return BCE + KLD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3icE_o0FKiJ"
      },
      "source": [
        "## 訓練データで学習を実行する関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3sBGwVKFFU6"
      },
      "source": [
        "log_interval = 50\n",
        "\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  for batch_idx, (data, _) in enumerate(train_loader): # 訓練データ\n",
        "    data = data.to(device) # データをGPUへ移動\n",
        "    optimizer.zero_grad() # 勾配を初期化\n",
        "    recon_batch, mu, logvar = model(data) # 前向き計算\n",
        "    loss = loss_function(recon_batch, data, mu, logvar) # negative ELBOの計算\n",
        "    loss.backward() # 勾配の計算\n",
        "    train_loss += loss.item()\n",
        "    optimizer.step() # パラメータの更新\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader),\n",
        "          loss.item() / len(data)))\n",
        "\n",
        "  print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "      epoch, train_loss / len(train_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9n5wtZJFfNj"
      },
      "source": [
        "## テストデータ上での評価をおこなう関数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ldHetdiFcav"
      },
      "source": [
        "def test(epoch):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  with torch.no_grad(): # 計算グラフを作らない\n",
        "    for i, (data, _) in enumerate(test_loader): # テストデータ\n",
        "      data = data.to(device)\n",
        "      recon_batch, mu, logvar = model(data)\n",
        "      test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "      if i == 0:\n",
        "        n = min(data.size(0), 8)\n",
        "        comparison = torch.cat([data[:n],\n",
        "                                recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
        "        save_image(comparison.cpu(),\n",
        "                   'reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('====> Test set loss: {:.4f}'.format(test_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNjgeXwXFj9w"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  train(epoch)\n",
        "  test(epoch)\n",
        "  with torch.no_grad():\n",
        "    sample = torch.randn(64, 20).to(device)\n",
        "    sample = model.decode(sample).cpu()\n",
        "    save_image(sample.view(64, 1, 28, 28),\n",
        "               'sample_' + str(epoch) + '.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn1UEMaK594b"
      },
      "source": [
        "## 全てのテストデータについて潜在表現を得る"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn4Bkahgn5bF"
      },
      "source": [
        "means = list()\n",
        "labels = list()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for i, (data, labels_batch) in enumerate(test_loader): # テストデータ\n",
        "    data = data.to(device)\n",
        "    _, means_batch, _ = model(data)\n",
        "    means.append(means_batch)\n",
        "    labels.append(labels_batch)\n",
        "labels = torch.cat(labels, 0).cpu().numpy()\n",
        "print(labels.shape)\n",
        "means = torch.cat(means, 0).cpu().numpy()\n",
        "print(means.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z0IyqxiyVBL"
      },
      "source": [
        "* https://plotly.com/python/pca-visualization/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgze-MeL6DC3"
      },
      "source": [
        "## テストデータの潜在表現をPCAで可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj-hZVDppCrE"
      },
      "source": [
        "pca = PCA(n_components=3)\n",
        "components = pca.fit_transform(means)\n",
        "\n",
        "total_var = pca.explained_variance_ratio_.sum() * 100\n",
        "\n",
        "fig = px.scatter_3d(\n",
        "    components, x=0, y=1, z=2, color=labels,\n",
        "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
        "    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n",
        ")\n",
        "fig.update_layout(\n",
        "    margin=dict(l=20, r=20, b=20, t=20),\n",
        "    width=900,\n",
        "    height=500\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rat4EnF6Gfe"
      },
      "source": [
        "## FashionMNISTのテストデータだけを読み込む"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1nOiwgBqFgx"
      },
      "source": [
        "batch_size = 200\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
        "fashion_test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.FashionMNIST('../data', train=False, download=True,\n",
        "                          transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnRuLr4X6KOj"
      },
      "source": [
        "## MNISTで学習させたVAEを使ってFashionMNISTの全てのテストデータの潜在表現を得る"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD4OSjdO1Gdd"
      },
      "source": [
        "fashion_means = list()\n",
        "fashion_labels = list()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for i, (data, labels_batch) in enumerate(fashion_test_loader): # テストデータ\n",
        "    data = data.to(device)\n",
        "    _, means_batch, _ = model(data)\n",
        "    fashion_means.append(means_batch)\n",
        "    fashion_labels.append(labels_batch)\n",
        "fashion_labels = torch.cat(fashion_labels, 0).cpu().numpy()\n",
        "print(fashion_labels.shape)\n",
        "fashion_means = torch.cat(fashion_means, 0).cpu().numpy()\n",
        "print(fashion_means.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLdl3dye6X5O"
      },
      "source": [
        "### FashionMNISTのラベルはプラス10しておく"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVsrhoVl3MWp"
      },
      "source": [
        "fashion_labels += 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWj_0V0C4iPh"
      },
      "source": [
        "fashion_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eII6Z9ER6ccG"
      },
      "source": [
        "## 両方のデータセットのテストデータの潜在表現を5000個ずつとって合併する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyoueR9d2iVg"
      },
      "source": [
        "all_labels = np.concatenate([labels[:5000], fashion_labels[:5000]])\n",
        "print(all_labels.shape)\n",
        "all_means = np.concatenate([means[:5000], fashion_means[:5000]])\n",
        "print(all_means.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OxgUHHX6iuo"
      },
      "source": [
        "## 合併したベクトル集合をPCAで可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-M9keaa2_AT"
      },
      "source": [
        "pca = PCA(n_components=3)\n",
        "components = pca.fit_transform(all_means)\n",
        "\n",
        "total_var = pca.explained_variance_ratio_.sum() * 100\n",
        "\n",
        "fig = px.scatter_3d(\n",
        "    components, x=0, y=1, z=2, color=all_labels,\n",
        "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
        "    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n",
        ")\n",
        "fig.update_layout(\n",
        "    margin=dict(l=20, r=20, b=20, t=20),\n",
        "    width=900,\n",
        "    height=500\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr_9g7ro6tAL"
      },
      "source": [
        "## k-meansで二種類のテストセットの潜在表現をどのくらい綺麗に分離できるか調べる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LtW5AzM3SSi"
      },
      "source": [
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(all_means)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSpeYHv05T-t"
      },
      "source": [
        "kmeans.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz7-Wcp75XaH"
      },
      "source": [
        "((1 - all_labels // 10) == kmeans.labels_).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eL7F_AG5cMY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}